{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146c6129",
   "metadata": {},
   "source": [
    "\n",
    "# HIT140 — Assignment 2 Pipeline (Investigation A)\n",
    "\n",
    "This notebook performs data cleaning **exactly per D1** and continues with the analyses required for **Assignment 2**, using both `dataset1.csv` and `dataset2.csv`.\n",
    "\n",
    "**Sections**\n",
    "1. Setup & imports  \n",
    "2. Load data  \n",
    "3. D1 Cleaning — datetime strategy, `habit` recode, derived features, outliers (no dropping), season labels, chronology checks  \n",
    "4. Align with `dataset2` (30‑min windows) and enrich `dataset1`  \n",
    "5. Descriptive analyses & figures  \n",
    "6. Inferential analysis (GLM Logit) for **Investigation A**  \n",
    "7. Save cleaned/enriched outputs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dd6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Setup & imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# Stats/ML\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "OUT_DIR = Path(\"./assign2_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.options.display.max_columns = 200\n",
    "pd.options.display.width = 160\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1694b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Load data (ensure the CSVs are in the same folder as this notebook)\n",
    "d1 = pd.read_csv(\"dataset1.csv\")\n",
    "d2 = pd.read_csv(\"dataset2.csv\")\n",
    "\n",
    "print(\"dataset1 shape:\", d1.shape)\n",
    "print(\"dataset2 shape:\", d2.shape)\n",
    "display(d1.head())\n",
    "display(d2.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5319e5",
   "metadata": {},
   "source": [
    "## 3. D1 Cleaning — Datetime strategy & derived fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9f56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parse as day-first and keep full datetimes\n",
    "def parse_dayfirst(s):\n",
    "    return pd.to_datetime(s, dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "for col in [\"start_time\", \"rat_period_start\", \"rat_period_end\", \"sunset_time\"]:\n",
    "    d1[col] = parse_dayfirst(d1[col])\n",
    "\n",
    "d2[\"time\"] = parse_dayfirst(d2[\"time\"])\n",
    "\n",
    "# Derived fields instead of discarding dates\n",
    "d1[\"date\"] = d1[\"start_time\"].dt.date\n",
    "d1[\"minutes_after_sunset\"] = (d1[\"start_time\"] - d1[\"sunset_time\"]).dt.total_seconds() / 60.0\n",
    "d1[\"minutes_after_rat_arrival\"] = d1[\"seconds_after_rat_arrival\"] / 60.0\n",
    "\n",
    "# Verify chronology per row: rat_period_start ≤ start_time ≤ rat_period_end\n",
    "d1[\"chronology_ok\"] = (\n",
    "    (d1[\"rat_period_start\"] <= d1[\"start_time\"]) &\n",
    "    (d1[\"start_time\"] <= d1[\"rat_period_end\"])\n",
    ")\n",
    "violations = (~d1[\"chronology_ok\"]).sum()\n",
    "print(\"Chronology violations:\", violations)\n",
    "d1.loc[~d1[\"chronology_ok\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d80544",
   "metadata": {},
   "source": [
    "### D1 Cleaning — `habit` recode (Unknowns, synonyms, order‑invariant composites, rare→Other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dcce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_token(t):\n",
    "    t = t.strip().lower()\n",
    "    t = re.sub(r\"[\\s\\-/]+\", \"_\", t)           # spaces/dashes/slashes -> underscore\n",
    "    t = re.sub(r\"[^a-z0-9_]\", \"\", t)            # drop other symbols\n",
    "    return t\n",
    "\n",
    "def is_coord_like(s):\n",
    "    return bool(re.search(r\"\\d+(\\.\\d+)?\\s*,\\s*\\d+(\\.\\d+)?\", s))\n",
    "\n",
    "def clean_habit_value(x):\n",
    "    if pd.isna(x):\n",
    "        return \"Unknown\"\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.isnumeric() or is_coord_like(s):\n",
    "        return \"Unknown\"\n",
    "    s = normalize_token(s)\n",
    "    # Canonicalize synonyms / spelling\n",
    "    s = s.replace(\"bats\", \"bat\")\n",
    "    s = s.replace(\"pick_and_bat\", \"pick_bat\")\n",
    "    s = s.replace(\"pickandbat\", \"pick_bat\")\n",
    "    s = s.replace(\"batfight\", \"bat_fight\")\n",
    "    s = s.replace(\"ratpick\", \"rat_pick\")\n",
    "    s = s.replace(\"pickbat\", \"pick_bat\")\n",
    "    s = s.replace(\"batrat\", \"bat_rat\")\n",
    "    s = s.replace(\"ratbat\", \"rat_bat\")\n",
    "    # Make composite labels order-invariant\n",
    "    s = s.replace(\"_and_\", \"_\")\n",
    "    parts = [p for p in s.split(\"_\") if p != \"\"]\n",
    "    if len(parts) > 1:\n",
    "        s = \"_\".join(sorted(parts))\n",
    "    return s or \"Unknown\"\n",
    "\n",
    "d1[\"habit_clean_raw\"] = d1[\"habit\"].apply(clean_habit_value)\n",
    "\n",
    "# Consolidate rare categories (frequency < ~10) into \"Other\"\n",
    "freq = d1[\"habit_clean_raw\"].value_counts(dropna=False)\n",
    "rare_threshold = 10\n",
    "rare_labels = set(freq[freq < rare_threshold].index.tolist())\n",
    "\n",
    "keep_labels = {\"fast\", \"pick\", \"rat\", \"bat\", \"bat_fight\", \"rat_pick\", \"Unknown\", \"other\"}\n",
    "\n",
    "def map_to_final_habit(lbl):\n",
    "    if lbl in keep_labels:\n",
    "        return lbl\n",
    "    if lbl in rare_labels:\n",
    "        return \"Other\"\n",
    "    return lbl\n",
    "\n",
    "d1[\"habit_clean\"] = d1[\"habit_clean_raw\"].apply(map_to_final_habit)\n",
    "\n",
    "# Show mapping preview\n",
    "display(pd.DataFrame({\n",
    "    \"original\": d1[\"habit\"].head(20),\n",
    "    \"clean_raw\": d1[\"habit_clean_raw\"].head(20),\n",
    "    \"clean_final\": d1[\"habit_clean\"].head(20)\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973e62f2",
   "metadata": {},
   "source": [
    "### D1 Cleaning — Outliers (keep rows; add transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cdf093",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Keep original; add transforms for skew reduction\n",
    "d1[\"bat_landing_to_food_log1p\"] = np.log1p(d1[\"bat_landing_to_food\"].clip(lower=0))\n",
    "d1[\"bat_landing_to_food_sqrt\"] = np.sqrt(d1[\"bat_landing_to_food\"].clip(lower=0))\n",
    "\n",
    "d1[[\"bat_landing_to_food\",\"bat_landing_to_food_log1p\",\"bat_landing_to_food_sqrt\"]].describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8e559",
   "metadata": {},
   "source": [
    "### D1 Cleaning — Season labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99543fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "season_map = {0: \"Winter\", 1: \"Spring\"}\n",
    "if \"season\" in d1.columns:\n",
    "    if np.issubdtype(d1[\"season\"].dtype, np.number):\n",
    "        d1[\"season_label\"] = d1[\"season\"].map(season_map).fillna(d1[\"season\"].astype(str))\n",
    "    else:\n",
    "        d1[\"season_label\"] = d1[\"season\"].astype(str)\n",
    "else:\n",
    "    d1[\"season_label\"] = d1[\"month\"].map(season_map).fillna(d1[\"month\"].astype(str))\n",
    "\n",
    "d1[\"season_label\"].value_counts(dropna=False).to_frame(\"count\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e8784",
   "metadata": {},
   "source": [
    "## 4. Enrich `dataset1` with 30‑min context from `dataset2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build 30-min windows for dataset2\n",
    "d2 = d2.sort_values(\"time\").copy()\n",
    "d2[\"window_start\"] = d2[\"time\"]\n",
    "d2[\"window_end\"] = d2[\"time\"] + pd.Timedelta(minutes=30)\n",
    "\n",
    "# Merge_asof to align each bat landing time to its window_start (backward), then filter within window\n",
    "d1 = d1.sort_values(\"start_time\").copy()\n",
    "enriched = pd.merge_asof(\n",
    "    d1,\n",
    "    d2[[\"window_start\",\"window_end\",\"bat_landing_number\",\"food_availability\",\"rat_minutes\",\"rat_arrival_number\"]],\n",
    "    left_on=\"start_time\",\n",
    "    right_on=\"window_start\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "enriched = enriched[(enriched[\"start_time\"] >= enriched[\"window_start\"]) & (enriched[\"start_time\"] < enriched[\"window_end\"])]\n",
    "print(\"enriched shape:\", enriched.shape)\n",
    "display(enriched.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6920502",
   "metadata": {},
   "source": [
    "## 5. Descriptive analyses & figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28158da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Numeric summary\n",
    "desc_cols = [\n",
    "    \"bat_landing_to_food\", \"bat_landing_to_food_log1p\", \"bat_landing_to_food_sqrt\",\n",
    "    \"seconds_after_rat_arrival\", \"minutes_after_rat_arrival\",\n",
    "    \"hours_after_sunset\", \"minutes_after_sunset\",\n",
    "    \"rat_arrival_number\", \"bat_landing_number\", \"food_availability\",\n",
    "    \"risk\", \"reward\"\n",
    "]\n",
    "summary = enriched[desc_cols].describe().T\n",
    "display(summary)\n",
    "summary.to_csv(OUT_DIR / \"summary_numeric.csv\")\n",
    "\n",
    "# Habit frequencies\n",
    "habit_counts = enriched[\"habit_clean\"].value_counts(dropna=False).rename_axis(\"habit_clean\").reset_index(name=\"n\")\n",
    "display(habit_counts.head(10))\n",
    "habit_counts.to_csv(OUT_DIR / \"habit_counts.csv\", index=False)\n",
    "\n",
    "# Season counts\n",
    "season_counts = enriched[\"season_label\"].value_counts(dropna=False).rename_axis(\"season_label\").reset_index(name=\"n\")\n",
    "display(season_counts)\n",
    "season_counts.to_csv(OUT_DIR / \"season_counts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Figures (matplotlib only — one per figure — no explicit colors)\n",
    "\n",
    "# Latency distributions\n",
    "plt.figure()\n",
    "plt.hist(enriched[\"bat_landing_to_food\"].dropna(), bins=40)\n",
    "plt.title(\"Distribution: bat_landing_to_food (seconds)\")\n",
    "plt.xlabel(\"Seconds from landing to food\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_bat_landing_to_food.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(enriched[\"bat_landing_to_food_log1p\"].dropna(), bins=40)\n",
    "plt.title(\"Distribution: log1p(bat_landing_to_food)\")\n",
    "plt.xlabel(\"log1p(seconds)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"hist_bat_landing_to_food_log1p.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# Risk-taking by season\n",
    "risk_by_season = enriched.groupby(\"season_label\")[\"risk\"].mean().reset_index()\n",
    "risk_by_season.to_csv(OUT_DIR / \"risk_by_season.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(risk_by_season[\"season_label\"], risk_by_season[\"risk\"])\n",
    "plt.title(\"Risk-taking rate by season\")\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Mean risk (1=risk-taking)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"bar_risk_by_season.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# Risk vs rat pressure (quartiles of rat_arrival_number)\n",
    "enriched[\"rat_arrival_bin\"] = pd.qcut(enriched[\"rat_arrival_number\"].fillna(0), q=4, duplicates=\"drop\")\n",
    "risk_by_ratbin = enriched.groupby(\"rat_arrival_bin\")[\"risk\"].mean().reset_index()\n",
    "risk_by_ratbin.to_csv(OUT_DIR / \"risk_by_ratarrival_bin.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "x = risk_by_ratbin[\"rat_arrival_bin\"].astype(str)\n",
    "y = risk_by_ratbin[\"risk\"]\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Risk-taking vs. rat_arrival_number (quartiles)\")\n",
    "plt.xlabel(\"rat_arrival_number quartiles\")\n",
    "plt.ylabel(\"Mean risk\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"bar_risk_vs_ratarrival_bins.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# Risk vs minutes_after_rat_arrival (quartiles)\n",
    "enriched[\"mar_bin\"] = pd.qcut(enriched[\"minutes_after_rat_arrival\"], q=4, duplicates=\"drop\")\n",
    "risk_by_marbin = enriched.groupby(\"mar_bin\")[\"risk\"].mean().reset_index()\n",
    "risk_by_marbin.to_csv(OUT_DIR / \"risk_by_minutes_after_rat_arrival_bin.csv\", index=False)\n",
    "\n",
    "plt.figure()\n",
    "x = risk_by_marbin[\"mar_bin\"].astype(str)\n",
    "y = risk_by_marbin[\"risk\"]\n",
    "plt.bar(x, y)\n",
    "plt.title(\"Risk-taking vs. minutes_after_rat_arrival (quartiles)\")\n",
    "plt.xlabel(\"minutes_after_rat_arrival quartiles\")\n",
    "plt.ylabel(\"Mean risk\")\n",
    "plt.xticks(rotation=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"bar_risk_vs_minutes_after_rat_arrival_bins.png\", dpi=180)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884af0c1",
   "metadata": {},
   "source": [
    "## 6. Inferential analysis — GLM Logit for Investigation A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6999a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing predictors\n",
    "model_df = enriched.dropna(subset=[\n",
    "    \"risk\", \"rat_arrival_number\", \"minutes_after_rat_arrival\",\n",
    "    \"minutes_after_sunset\", \"bat_landing_to_food_log1p\", \"season_label\", \"habit_clean\"\n",
    "]).copy()\n",
    "\n",
    "# Categorical types\n",
    "model_df[\"season_label\"] = model_df[\"season_label\"].astype(\"category\")\n",
    "model_df[\"habit_clean\"] = model_df[\"habit_clean\"].astype(\"category\")\n",
    "\n",
    "# Model: does rat pressure/time since rat arrival predict risk-taking?\n",
    "formula = (\n",
    "    \"risk ~ rat_arrival_number + minutes_after_rat_arrival + minutes_after_sunset \"\n",
    "    \"+ bat_landing_to_food_log1p + C(season_label) + C(habit_clean)\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    logit_model = smf.glm(formula=formula, data=model_df, family=sm.families.Binomial()).fit()\n",
    "    print(logit_model.summary())\n",
    "    # Save text and odds ratios\n",
    "    with open(OUT_DIR / \"logistic_results.txt\", \"w\") as f:\n",
    "        f.write(logit_model.summary().as_text())\n",
    "    params = logit_model.params\n",
    "    conf = logit_model.conf_int()\n",
    "    or_table = pd.DataFrame({\n",
    "        \"OR\": np.exp(params),\n",
    "        \"CI_low\": np.exp(conf[0]),\n",
    "        \"CI_high\": np.exp(conf[1]),\n",
    "        \"pvalue\": logit_model.pvalues\n",
    "    })\n",
    "    display(or_table)\n",
    "    or_table.to_csv(OUT_DIR / \"logistic_odds_ratios.csv\")\n",
    "except Exception as e:\n",
    "    print(\"Model fitting failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6be6ca9",
   "metadata": {},
   "source": [
    "## 7. Save cleaned/enriched datasets and a compact data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf3555",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save cleaned/intermediate/final tables\n",
    "d1.to_csv(OUT_DIR / \"dataset1_cleaned.csv\", index=False)\n",
    "d2.to_csv(OUT_DIR / \"dataset2_parsed.csv\", index=False)\n",
    "enriched.to_csv(OUT_DIR / \"dataset1_enriched.csv\", index=False)\n",
    "\n",
    "# Data dictionary for engineered fields\n",
    "data_dict = pd.DataFrame({\n",
    "    \"column\": [\n",
    "        \"habit_clean_raw\", \"habit_clean\", \"date\",\n",
    "        \"minutes_after_sunset\", \"minutes_after_rat_arrival\",\n",
    "        \"chronology_ok\", \"bat_landing_to_food_log1p\", \"bat_landing_to_food_sqrt\",\n",
    "        \"season_label\", \"window_start\", \"window_end\",\n",
    "        \"bat_landing_number\", \"food_availability\", \"rat_minutes\", \"rat_arrival_number\"\n",
    "    ],\n",
    "    \"description\": [\n",
    "        \"Normalized habit string before rare-category consolidation\",\n",
    "        \"Final habit category after synonym, order-invariant normalization, rare->Other\",\n",
    "        \"Calendar date of landing\",\n",
    "        \"Minutes from sunset to landing (derived from timestamps)\",\n",
    "        \"Minutes from rat arrival to bat landing (derived from given seconds)\",\n",
    "        \"True if start_time lies within [rat_period_start, rat_period_end]\",\n",
    "        \"log1p transform of bat_landing_to_food for skew reduction\",\n",
    "        \"sqrt transform of bat_landing_to_food for skew reduction\",\n",
    "        \"Season string label (mapped from season/month)\",\n",
    "        \"Start of matched 30-min observation window from dataset2\",\n",
    "        \"End of matched 30-min observation window from dataset2\",\n",
    "        \"Total bat landings in that 30-min interval (context/crowding)\",\n",
    "        \"Estimated food availability in that interval\",\n",
    "        \"Total minutes rats were present in that interval\",\n",
    "        \"Rat arrival count in that interval (rat pressure)\"\n",
    "    ]\n",
    "})\n",
    "data_dict.to_csv(OUT_DIR / \"engineered_data_dictionary.csv\", index=False)\n",
    "\n",
    "print(\"Written outputs to:\", OUT_DIR.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
